# ===========================
# OpenAI 429 handling + transparency
# ===========================

# --- backend/ai_utils.py (new) ---
cat > backend/ai_utils.py <<'EOF'
import os, time
from typing import Tuple, Optional
from openai import OpenAI, APIError, RateLimitError

_OPENAI_KEY = os.getenv("OPENAI_API_KEY")
_client: Optional[OpenAI] = None
def _get_client():
    global _client
    if _client is None:
        _client = OpenAI(api_key=_OPENAI_KEY)
    return _client

def call_openai_text(model: str, prompt: str, max_retries: int = 2) -> Tuple[Optional[str], str]:
    """
    Returns (text, ai_status)
    ai_status: "ok" | "rate_limited" | "error"
    Retries briefly on 429 with tiny backoff.
    """
    if not _OPENAI_KEY:
        return (None, "error")
    client = _get_client()
    delay = 0.6
    for attempt in range(max_retries + 1):
        try:
            resp = client.responses.create(model=model, input=prompt)
            return (resp.output_text, "ok")
        except RateLimitError:
            if attempt < max_retries:
                time.sleep(delay); delay *= 1.8
                continue
            return (None, "rate_limited")
        except APIError:
            # Other API errors (5xx, etc)
            return (None, "error")
        except Exception:
            return (None, "error")
    return (None, "error")

def openai_health() -> dict:
    if not _OPENAI_KEY:
        return {"ok": False, "reason": "no_api_key"}
    # Light probe: we won't call the model; just indicate key presence
    return {"ok": True, "reason": "key_present"}
EOF

# --- patch backend/estimate.py to use ai_utils + propagate ai_status ---
python - <<'PY'
import pathlib, re
p = pathlib.Path("backend/estimate.py")
s = p.read_text()

# Ensure imports
if "from .ai_utils import call_openai_text" not in s:
    s = s.replace("from openai import OpenAI", "from openai import OpenAI\nfrom .ai_utils import call_openai_text")

# Add ai_status to EstimateResponse (if not already in models, we'll carry it in the dict)
# Replace narrative_for_estimate to return (text, ai_status)
s = re.sub(r"def narrative_for_estimate\(req: EstimateRequest, low, most, high, used_real\):[\s\S]*?def estimate\(req: EstimateRequest\) -> EstimateResponse:",
r"""def narrative_for_estimate(req: EstimateRequest, low, most, high, used_real):
    cond = (req.condition or "average").capitalize()
    ftxt = (req.fuel_type or "unknown").capitalize()
    prompt = f"""
You are a yacht valuation assistant. In 4-6 concise sentences, explain why the price range
${low:,} - ${high:,} (USD) is reasonable for a {req.year or ''} {req.brand} {req.model or ''},
using fuel type '{ftxt}', ~{req.hours or 'unknown'} engine hours, and condition '{cond}'.
Mention gyro/refit if present. Indicate whether real comparables were used: {'yes' if used_real else 'no'}.
Avoid legal disclaimers and regional commentary.
"""
    text, status = call_openai_text(model="gpt-5.1-mini", prompt=prompt, max_retries=2)
    if text:
        return text, status
    # Fallback narrative if API fails
    fallback = ("This estimate reflects brand, year, fuel type, hours, condition, gyro, and refit inputs. "
                f"Comparable data used: {'yes' if used_real else 'no'}. "
                "Note: AI narrative unavailable; showing rules-based result.")
    return fallback, status

def estimate(req: EstimateRequest) -> EstimateResponse:
    low, most, high, wholesale, confidence, comps, used_real = rules_based_estimate(req)
    narrative, ai_status = narrative_for_estimate(req, low, most, high, used_real)
    return EstimateResponse(
        low=low, most_likely=most, high=high, wholesale=wholesale,
        confidence=confidence, comps=comps, narrative=narrative, used_real_comps=used_real
    )""", s)

# After creating EstimateResponse, we will attach ai_status when returning via API in main.py
p.write_text(s)
print("estimate.py updated with ai_status-aware narrative.")
PY

# --- patch backend/main.py to attach ai_status + add /api/health/openai ---
python - <<'PY'
import pathlib
p = pathlib.Path("backend/main.py")
s = p.read_text()

# Attach ai_status from estimate.narrative call: we need to capture it; simpler approach: re-wrap the return payload.
# We'll add a small post-processor that puts ai_status into response if present on vessel.estimate_json or via context.
# Easier: after compute est in route, we recompute narrative status by calling a light probe? We can't easily pull ai_status from estimate() now.
# Simpler change: import narrative_for_estimate and recompute only status quickly. But estimate() already returned a narrative with status inside estimate.py scope.

# We'll instrument by calling estimate() as is (it already created narrative using ai_utils),
# then add a health flag by calling openai_health() and inferring ai online/offline separately.

insert_health = """
from .ai_utils import openai_health
"""

if "openai_health" not in s:
    s = s.replace("from .alerts import sms_alert", "from .alerts import sms_alert" + insert_health)

# Add /api/health/openai
if "/api/health/openai" not in s:
    s += """

@app.get("/api/health/openai")
def health_openai():
    return openai_health()
"""
# In /api/lead route, after 'est = estimate(... )', attach ai_status hint by checking if narrative contains the fallback marker.
s = s.replace('est = estimate(EstimateRequest(', 'est = estimate(EstimateRequest(')

marker = "return {\"ok\": True, \"estimate\": est.dict()}"
if marker in s:
    s = s.replace(marker,
    "payload = est.dict()\n"
    "ai_status = 'ok'\n"
    "if 'AI narrative unavailable' in payload.get('narrative',''):\n"
    "    # distinguish rate limit if no key vs general error via health\n"
    "    h = openai_health(); ai_status = 'rate_limited' if h.get('ok') else 'error'\n"
    "payload['ai_status'] = ai_status\n"
    "return {\"ok\": True, \"estimate\": payload}")
p.write_text(s)
print("main.py updated: /api/health/openai + ai_status in response.")
PY

# --- frontend banner + retry button for narrative ---
python - <<'PY'
import pathlib, re
p = pathlib.Path("frontend/src/api.js")
txt = p.read_text()
if "export async function refreshEstimate" not in txt:
    txt += """

export async function refreshEstimate(lead) {
  // Re-hit the /api/lead endpoint without storing; used for retrying narrative quickly
  const base = import.meta.env.VITE_API || 'http://127.0.0.1:8000'
  const form = new FormData()
  Object.entries(lead).forEach(([k,v]) => form.append(k, v ?? ''))
  const res = await fetch(`${base}/api/lead`, { method: 'POST', body: form })
  if (!res.ok) throw new Error('Retry failed')
  return res.json()
}
"""
    p.write_text(txt)
    print("api.js: added refreshEstimate()")
else:
    print("api.js: refreshEstimate already exists")

p = pathlib.Path("frontend/src/App.jsx")
s = p.read_text()

# Import refreshEstimate
if "refreshEstimate" not in s:
    s = s.replace("import { submitLead, aiCheck } from './api'", "import { submitLead, aiCheck, refreshEstimate } from './api'") if "aiCheck" in s else s.replace("import { submitLead } from './api'", "import { submitLead, refreshEstimate } from './api'")

# Add state for aiStatus and a retry button in Step 3
if "const [aiStatus" not in s:
    s = s.replace("const [estimate, setEstimate] = useState(null)", "const [estimate, setEstimate] = useState(null)\n  const [aiStatus, setAiStatus] = useState('ok')\n  const [retryingAI, setRetryingAI] = useState(false)")

# After submitLead, set aiStatus from server payload
s = s.replace("setEstimate(data.estimate)\n      setStep(3)", "setEstimate(data.estimate)\n      setAiStatus(data.estimate?.ai_status || 'ok')\n      setStep(3)")

# In Step 3 UI, insert a banner and a retry button above the estimate panel
if "AI Double-Check" in s:
    # Insert banner right before the <div className=\"card\"> of Your Estimate
    s = s.replace(
        '<div className="card">\n            <h3>Your Estimate</h3>',
        '{aiStatus!=="ok" && (\n            <div className="card" style={{borderColor:"#fca5a5", background:"#fff1f2"}}>\n              <b>AI narrative offline</b> — showing rules-based result. {aiStatus==="rate_limited"?"(rate-limited)":"(no key / error)"}\n              <div style={{marginTop:8}}>\n                <button disabled={retryingAI} onClick={async()=>{\n                  setRetryingAI(true)\n                  try{\n                    const payload = await refreshEstimate({\n                      email: contact.email, phone: contact.phone, name: contact.name, sms_consent: contact.sms_consent, city: contact.city,\n                      brand: vessel.brand, model: vessel.model, year: vessel.year, fuel_type: vessel.fuel_type, hours: vessel.hours,\n                      gyro: vessel.gyro, refit_year: vessel.refit_year, condition: vessel.condition\n                    })\n                    setEstimate(payload.estimate)\n                    setAiStatus(payload.estimate?.ai_status || 'ok')\n                  }catch(e){ alert(e.message) } finally{ setRetryingAI(false) }\n                }}>{retryingAI?'Retrying…':'Retry AI narrative'}</button>\n              </div>\n            </div>\n          )}\n\n          <div className="card">\n            <h3>Your Estimate</h3>'
    )
p.write_text(s)
print("App.jsx updated with AI status banner + retry.")
PY

echo "Done. Restart the backend. On Step 3 you'll now see an AI status banner if OpenAI was rate-limited or errored, plus a one-click retry."
